# 游戏智能NPC角色训练

## 介绍

这是一个探索性的项目，目的是测试在当前的游戏中，如果希望NPC具有智能对话的能力，是否可以用现有开源方案的大语言模型实现。我相信ChatGPT3.5和ChatGPT4.0通过微调是可以实现一个可用的智能NPC对象的，但是ChatGPT对于网络游戏存在以下问题，使其暂时还不适合实际生产环境。

1. ChatGPT的finetune和query调用都是有很高成本的，这对于独立游戏或者免费运营的游戏是很大的资金负担。
2. ChatGPT的响应速度和稳定性可能受到大环境影响，不能保持稳定
3. ChatGPT可能存在数据泄露的风险

因此使用独立部署的大语言模型是更加的选择，当然目前的开源的模型无法达到ChatGPT的智能程度，尤其是对于非英文语言，需要有大量的调优工作。

## 项目说明

本项目使用了一款网络游戏的世界观和文案设定，形成了10K的问题集，大约654K汉字量，去调优Vicuna模型，调优在单3090 GPU的机器上运营，因此选用了7B参数量的模型做基础。具体用到的模型如下：

1. [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) 这是一个优秀的中文低资源的llama+lora方案，这个模型使用了guanaco_belle数据集在vicuna标准模型上做微调，使其支持中文，但因为中文的语料不足够多，使其中文的语言能力仍有缺陷。
2. [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) 这是另一个优秀的中文LLaMA/Alpaca权重模型，在4月28日发布了7B的优化版本，参见：[民间版中文羊驼模型（Plus）v3.0](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v3.0)

## 环境准备

创建项目的可执行环境

```bash
cd $PROJECT_DIR
git clone https://github.com/wangqi/game-npc-vicuna.git
cd game-npc-vicuna
conda create -n gamenpc python=3.10
conda activate gamenpc
pip install -r ./requirements.txt
```

## 微调步骤

本项目 `data/data.json` 文件是游戏世界观的训练文件，此游戏是关于少女猎人在被龙兽毁灭的世界上冒险的故事。其中包含了60几名角色和数万字的对话数据。

对项目微调的步骤如下：

1. 选择基础模型。我比较了 [decapoda-research/llama-7b-hf](https://huggingface.co/decapoda-research/llama-7b-hf) 和 [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b) 这两个基础模型。decapoda-research的模型虽然更普及，但它使用了旧的 `LLaMATokenizer`，这会让系统抛出一个警告。因此我选用了较新的huggyllama基础模型。选择7B参数量是为了单卡3090做fine tune更方便。如果用13B版本效果应该会更好。

2. 合并中文Lora权重。为了节约训练时间，可以将现有的中文LoRA模型权重合并到基础模型之上，比如Chinese-Vicuna微调的409M的权重和/或Chinese-LLaMA-Alpaca使用了18M的权重。权重可以叠加。不过这些训练的语料不是针对游戏定制的，当训练的游戏文本量过少时，可能造成最终微调的模型出现游戏世界观之外的对话内容，即无法融入游戏世界内。解决这个问题需要准备大量的游戏背景语料对基础模型进行训练。
   执行以下命令合并模型：

   ```bash
   ./merge_base.sh [base_model] [lora_weight] [output_path]
   ```

   其中base_model和lora_weight可以是huggingface的model id，也可以是本地模型的路径。如果不指定，则使用默认的设置。

   例如：

   ```bash
   # Merge the llama-7b base model with Chinese-Vicuna-7b
   ./merge_base.sh huggyllama/llama-7b Facico/Chinese-Vicuna-lora-7b-3epoch-belle-and-guanaco models/huggyllama_chinese-vicuna-7b-3epock-belle-guanaco/
   
   # Merge the Chinese-alpaca-plus-7b with the model generated by last step.
   ./merge_base.sh models/huggyllama_chinese-vicuna-7b-3epock-belle-guanaco/ ziqingyang/chinese-alpaca-plus-lora-7b models/huggyllama_chinese-vicuna-7b-3epock-belle-guanaco-chinese-llama-alpaca/
   
   # The final base model is:
   ls -l models/huggyllama_chinese-vicuna-7b-3epock-belle-guanaco-chinese-llama-alpaca/
   ```

3. 【todo】

## 注意事项

因为Facebook的[LLaMA](https://github.com/facebookresearch/llama)模型禁止商用，所以本项目暂时无法用于商业游戏项目，需要等待Facebook更新授权或者替换其他可商用模型。

## Citation

```
@inproceedings{leng2023chinese-vicuna,
  title={Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model},
  author={Chenghao Fan, Zhenyi Lu and Jie Tian},
  url={https://github.com/Facico/Chinese-Vicuna},
  year={2023}
}
```

```
@article{chinese-llama-alpaca,
      title={Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca}, 
      author={Cui, Yiming and Yang, Ziqing and Yao, Xin},
      journal={arXiv preprint arXiv:2304.08177},
      url={https://arxiv.org/abs/2304.08177},
      year={2023}
}
```