{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T17:36:46.396126500Z",
     "start_time": "2023-05-08T17:36:26.857008Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../../models/game_npc_vicuna_huntress/ggml-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 49954\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 13448820.20 KB\n",
      "llama_model_load_internal: mem required  = 14925.61 MB (+ 1026.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1024.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "local_path = \"../../models/game_npc_vicuna_huntress/ggml-f16.bin\"\n",
    "llm = LlamaCpp(model_path=local_path, verbose=True, n_batch=256, temperature=0.3, n_ctx=2048, use_mmap=False, stop=[\"###\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T17:53:12.249565600Z",
     "start_time": "2023-05-08T17:53:11.979938400Z"
    }
   },
   "outputs": [],
   "source": [
    "from retrivers.llama_time_weighted_retriever import LlamaTimeWeightedVectorStoreRetriever\n",
    "from vectorestores.chroma import EnhancedChroma\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    embeddings_model = LlamaCppEmbeddings(model_path=local_path)\n",
    "    vs = EnhancedChroma(embedding_function=embeddings_model)\n",
    "    return LlamaTimeWeightedVectorStoreRetriever(vectorstore=vs, other_score_keys=[\"importance\"], k=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T17:57:44.647867Z",
     "start_time": "2023-05-08T17:53:17.689900600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../../models/game_npc_vicuna_huntress/ggml-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 49954\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  68.20 KB\n",
      "llama_model_load_internal: mem required  = 14925.61 MB (+ 2052.00 MB per state)\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from generative_agents.llama_generative_agent import LlamaGenerativeAgent\n",
    "from generative_agents.llama_memory import LlamaGenerativeAgentMemory\n",
    "\n",
    "aoliweiya_memory = LlamaGenerativeAgentMemory(\n",
    "    llm=llm,\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    reflection_threshold=8, # we will give this a relatively low number to show how reflection works\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T17:57:45.740110800Z",
     "start_time": "2023-05-08T17:57:44.647867Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../../models/game_npc_vicuna_huntress/ggml-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 49954\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  14.06 KB\n",
      "llama_model_load_internal: mem required  = 4984.75 MB (+ 2052.00 MB per state)\n",
      "error loading model: llama.cpp: tensor 'layers.6.attention_norm.weight' is missing from model\n",
      "llama_init_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Could not load Llama model from path: ../../models/game_npc_vicuna_huntress/ggml-f16.bin",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m/mnt/e/anaconda_envs/envs/gamenpc/lib/python3.10/site-packages/langchain/embeddings/llamacpp.py:78\u001B[0m, in \u001B[0;36mLlamaCppEmbeddings.validate_environment\u001B[0;34m(cls, values)\u001B[0m\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_cpp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Llama\n\u001B[0;32m---> 78\u001B[0m     values[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclient\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mLlama\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_ctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_ctx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_parts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_parts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[43m        \u001B[49m\u001B[43mf16_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mf16_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     84\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_all\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_mlock\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_mlock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n",
      "File \u001B[0;32m/mnt/e/anaconda_envs/envs/gamenpc/lib/python3.10/site-packages/llama_cpp/llama.py:159\u001B[0m, in \u001B[0;36mLlama.__init__\u001B[0;34m(self, model_path, n_ctx, n_parts, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, verbose)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx \u001B[38;5;241m=\u001B[39m llama_cpp\u001B[38;5;241m.\u001B[39mllama_init_from_file(\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\n\u001B[1;32m    157\u001B[0m )\n\u001B[0;32m--> 159\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlora_path:\n",
      "\u001B[0;31mAssertionError\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m aoliweiya \u001B[38;5;241m=\u001B[39m LlamaGenerativeAgent(\n\u001B[1;32m      2\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m奥莉薇娅\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m     age\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25\u001B[39m,\n\u001B[1;32m      4\u001B[0m     traits\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m坚强, 干练, 大姐姐\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;66;03m# You can add more persistent traits here\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     status\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m导师\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;66;03m# When connected to a virtual world, we can have the characters update their status\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m     memory_retriever\u001B[38;5;241m=\u001B[39m\u001B[43mcreate_new_memory_retriever\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m      7\u001B[0m     llm\u001B[38;5;241m=\u001B[39mllm,\n\u001B[1;32m      8\u001B[0m     memory\u001B[38;5;241m=\u001B[39maoliweiya_memory,\n\u001B[1;32m      9\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     10\u001B[0m )\n",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m, in \u001B[0;36mcreate_new_memory_retriever\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_new_memory_retriever\u001B[39m():\n\u001B[0;32m----> 5\u001B[0m     embeddings_model \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaCppEmbeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     vs \u001B[38;5;241m=\u001B[39m EnhancedChroma(embedding_function\u001B[38;5;241m=\u001B[39membeddings_model)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m LlamaTimeWeightedVectorStoreRetriever(vectorstore\u001B[38;5;241m=\u001B[39mvs, other_score_keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m\"\u001B[39m], k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m14\u001B[39m)\n",
      "File \u001B[0;32m/mnt/e/anaconda_envs/envs/gamenpc/lib/python3.10/site-packages/pydantic/main.py:339\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/mnt/e/anaconda_envs/envs/gamenpc/lib/python3.10/site-packages/pydantic/main.py:1102\u001B[0m, in \u001B[0;36mpydantic.main.validate_model\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/mnt/e/anaconda_envs/envs/gamenpc/lib/python3.10/site-packages/langchain/embeddings/llamacpp.py:98\u001B[0m, in \u001B[0;36mLlamaCppEmbeddings.validate_environment\u001B[0;34m(cls, values)\u001B[0m\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(\n\u001B[1;32m     93\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import llama-cpp-python library. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     94\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install the llama-cpp-python library to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     95\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse this embedding model: pip install llama-cpp-python\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     96\u001B[0m     )\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m---> 98\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not load Llama model from path: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m values\n",
      "\u001B[0;31mNameError\u001B[0m: Could not load Llama model from path: ../../models/game_npc_vicuna_huntress/ggml-f16.bin"
     ]
    }
   ],
   "source": [
    "aoliweiya = LlamaGenerativeAgent(\n",
    "    name=\"奥莉薇娅\",\n",
    "    age=25,\n",
    "    traits=\"坚强, 干练, 大姐姐\", # You can add more persistent traits here\n",
    "    status=\"导师\", # When connected to a virtual world, we can have the characters update their status\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    llm=llm,\n",
    "    memory=aoliweiya_memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T17:58:14.511604500Z",
     "start_time": "2023-05-08T17:57:45.742794800Z"
    }
   },
   "outputs": [],
   "source": [
    "print(aoliweiya.get_summary(force_refresh=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T17:58:27.231866900Z",
     "start_time": "2023-05-08T17:58:14.511604500Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can add memories directly to the memory object\n",
    "aoliweiya_observations = [\n",
    "    \"猎人队长通过试训\",\n",
    "    \"雪莉加入队伍\",\n",
    "    \"妮可前来报名，但是晕倒在帐篷外\",\n",
    "    \"艾丽娜怀疑妮可的能力，不希望她加入猎人小队\",\n",
    "    \"猎人队长正在招聘新队员\",\n",
    "    \"城外发现三只四级龙兽\",\n",
    "    \"雪莉希望出城扫荡龙兽\",\n",
    "]\n",
    "for observation in aoliweiya_observations:\n",
    "    aoliweiya_memory.add_memory(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that Aoliweiya has 'memories', their self-summary is more descriptive, though still rudimentary.\n",
    "# We will see how this summary updates after more observations to create a more rich description.\n",
    "print(aoliweiya.get_summary(force_refresh=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aoliweiya.generate_reaction(\"猎人队长申请出城扫荡龙兽\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoliweiya.generate_dialogue(\"领队大人\", \"你的小队准备好了吗？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
